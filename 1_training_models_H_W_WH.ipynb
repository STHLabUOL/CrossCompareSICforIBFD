{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Hammerstein-, Wiener- and WienerHammerstein model on any type of data, with or without linear preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from models import NeuralNetGlobalHammer, NeuralNetGlobalWiener, NeuralNetGlobalHammerWiener, cmplxMeanSquaredError\n",
    "from util import load_signal_data, SIestimationLinear\n",
    "from config import *\n",
    "\n",
    "import pickle\n",
    "import fullduplex as fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = 'H' # R/H/W\n",
    "MODEL_TYPE = 'WH' # H/W/WH\n",
    "LIN_PREPROCESSING = False\n",
    "\n",
    "CHANNEL_LEN = 13\n",
    "TRAINING_RATIO = 0.9\n",
    "\n",
    "EXPORT_RESULTS = False\n",
    "PATH_EXPORT_RESULTS = 'comparison_results/new/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Signal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_TYPE == 'R':\n",
    "\n",
    "    matFile = sio.loadmat(PATH_DATA_REAL)\n",
    "\n",
    "    x_train_batched = matFile['txSamples']\n",
    "    x_train_batched = np.expand_dims(x_train_batched, 0)\n",
    "    x_train_batched = np.expand_dims(x_train_batched, 0)\n",
    "    x_train_batched = x_train_batched[:,:,:TOTAL_SIGNAL_LENGTH,:]\n",
    "    x_train_batched = x_train_batched/np.std(x_train_batched)\n",
    "    x_train_batched = tf.convert_to_tensor(x_train_batched)\n",
    "\n",
    "    y_train_batched = matFile['analogResidual']\n",
    "    y_train_batched = np.expand_dims(y_train_batched, 0)\n",
    "    y_train_batched = np.expand_dims(y_train_batched, 0)\n",
    "    y_train_batched = y_train_batched[:,:,:TOTAL_SIGNAL_LENGTH,:]\n",
    "    y_train_batched = y_train_batched[:,:,CHANNEL_LEN-1:,:]\n",
    "    y_train_batched = tf.convert_to_tensor(y_train_batched)\n",
    "\n",
    "    noise = np.squeeze(matFile['noiseSamples'], axis=1)\n",
    "\n",
    "elif DATA_TYPE == 'H' or DATA_TYPE == 'W':\n",
    "\n",
    "    DATA_DIR_ROOT = PATH_DATA_SYNTH_HAMMERSTEIN if DATA_TYPE == 'H' else PATH_DATA_SYNTH_WIENER\n",
    "    x_train, x_train_batched, y_train, y_train_batched, noise_train = load_signal_data(DATA_DIR_ROOT, noSignals=1, norm_y='none', norm_x='var', seq_len=TOTAL_SIGNAL_LENGTH, filter_len=CHANNEL_LEN, with_noise=True)\n",
    "\n",
    "    noise = noise_train\n",
    "    noise = np.squeeze(noise)\n",
    "\n",
    "else:\n",
    "    raise ValueError('DATA_TYPE must be either \"R\", \"H\", or \"W\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split into training- and test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSamples = int(np.floor(y_train_batched.shape[2]*TRAINING_RATIO))\n",
    "x_test_batched = x_train_batched[:,:,trainingSamples:,:]\n",
    "y_test_batched = y_train_batched[:,:,trainingSamples:,:]\n",
    "x_train_batched = x_train_batched[:,:,:trainingSamples+CHANNEL_LEN-1,:] # indexing is unintuitive at first glance due to the different lengths of x and y signals\n",
    "y_train_batched = y_train_batched[:,:,:trainingSamples,:]\n",
    "\n",
    "\n",
    "# consider additional offset according to balatsoukas\n",
    "if DATA_TYPE == 'R':    \n",
    "    offset = np.maximum(DATA_OFFSET-int(np.ceil(CHANNEL_LEN/2)),1) # according to Balatsoukas, exact logic is unclear...\n",
    "    x_train_batched = x_train_batched[:,:,:-offset,:]\n",
    "    x_test_batched = x_test_batched[:,:,:-offset,:]\n",
    "    y_train_batched = y_train_batched[:,:,offset:,:]\n",
    "    y_test_batched = y_test_batched[:,:,offset:,:]\n",
    "\n",
    "    # additional mean-removal in receive signal\n",
    "    y_train_batched -= np.mean(y_train_batched)\n",
    "    y_test_batched -= np.mean(y_test_batched)\n",
    "\n",
    "\n",
    "y_test_batched_orig = tf.identity(y_test_batched) # needed later for eval, untouched by lin preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional linear preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LIN_PREPROCESSING:\n",
    "\n",
    "    # For train data\n",
    "    h = SIestimationLinear(x_train_batched[0,0,:,0].numpy(), y_train_batched[0,0,:,0].numpy(), CHANNEL_LEN)\n",
    "    y_train_lin = fd.SIcancellationLinear(x_train_batched[0,0,:,0].numpy(), h, {})[CHANNEL_LEN-1:]\n",
    "    y_train_batched = y_train_batched[0,0,:,0].numpy() - y_train_lin\n",
    "    y_train_batched = np.expand_dims(y_train_batched, 0)\n",
    "    y_train_batched = np.expand_dims(y_train_batched, 0)\n",
    "    y_train_batched = tf.convert_to_tensor(np.expand_dims(y_train_batched, 3))\n",
    "\n",
    "    # For test data\n",
    "    y_test_lin = fd.SIcancellationLinear(x_test_batched[0,0,:,0].numpy(), h, {})[CHANNEL_LEN-1:]\n",
    "    y_test_batched = y_test_batched[0,0,:,0].numpy() - y_test_lin\n",
    "    y_test_batched = np.expand_dims(y_test_batched, 0)\n",
    "    y_test_batched = np.expand_dims(y_test_batched, 0)\n",
    "    y_test_batched = tf.convert_to_tensor(np.expand_dims(y_test_batched, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == 'H':\n",
    "   neuralNet = NeuralNetGlobalHammer(filterLen=CHANNEL_LEN, expected_SI_power_dB=10*np.log10(np.var(y_train_batched)))\n",
    "elif MODEL_TYPE == 'W':\n",
    "   neuralNet = NeuralNetGlobalWiener(filterLen=CHANNEL_LEN, expected_SI_power_dB=10*np.log10(np.var(y_train_batched)))\n",
    "elif MODEL_TYPE == 'WH':\n",
    "   neuralNet = NeuralNetGlobalHammerWiener(filterLen=CHANNEL_LEN, expected_SI_power_dB=10*np.log10(np.var(y_train_batched)))\n",
    "else:\n",
    "   raise ValueError('MODEL_TYPE must be \"H\", \"W\", or \"WH\"')\n",
    "\n",
    "\n",
    "optim = keras.optimizers.Adam(learning_rate=0.003, amsgrad=False, beta_1=0.9) \n",
    "neuralNet.compile(loss=cmplxMeanSquaredError(),    \n",
    "              optimizer=optim, # Einzelsignal 0.01,  Zehn Signale lr=0.001 bei batchsize=1\n",
    "              metrics=['MeanSquaredError'], run_eagerly=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = neuralNet.fit(x_train_batched, y_train_batched,                    \n",
    "          batch_size=100,\n",
    "          epochs=8000,\n",
    "          verbose=1,  \n",
    "          validation_split=0.0,\n",
    "          shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, mse_test = neuralNet.evaluate(x_test_batched, y_test_batched)\n",
    "mse_test_dB = np.round((10*np.log10(mse_test))*10)/10\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.tight_layout()\n",
    "\n",
    "finalMSE = history.history['mean_squared_error'][-1]\n",
    "plt.plot(10*np.log10(history.history['mean_squared_error']), label=f'MSE, Final: {np.round((10*np.log10(finalMSE))*10)/10}dB')\n",
    "plt.axhline(10*np.log10(mse_test), label=f'MSE (Test): {mse_test_dB}dB', linestyle='--', color='tab:orange')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylabel('MSE [dB]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Balatsoukas-Style PSD Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'normal',\n",
    "        'weight' : 'regular',\n",
    "        'size'   : 14}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "\n",
    "\n",
    "# Plot PSD and get signal powers\n",
    "scalingConst = np.array([PSD_SCALING_CONST]) # as per balatsoukas measurements, set equally for comparability\n",
    "yVarTest = np.var(y_test_batched[0, 0, CHANNEL_LEN-1:, 0])\n",
    "yPred = neuralNet(x_test_batched)[0, 0, CHANNEL_LEN-1:, 0] \n",
    "\n",
    "if not LIN_PREPROCESSING:\n",
    "    y_test_lin = 0*y_test_batched[0,0,:,0]\n",
    "\n",
    "\n",
    "fig, noisePower, yTestPower, yTestLinCancPower, yTestNonLinCancPower = fd.plotPSD(y_test_batched_orig[0, 0, CHANNEL_LEN-1:, 0]/np.sqrt(scalingConst), \n",
    "                                                                        y_test_lin[(CHANNEL_LEN-1):]/np.sqrt(scalingConst), \n",
    "                                                                        yPred/(np.sqrt(scalingConst)*np.sqrt(yVarTest)), # anticipate that the function subtracts prediction from normalized SI signal\n",
    "                                                                        noise/np.sqrt(scalingConst), \n",
    "                                                                        {'hSILen': CHANNEL_LEN, 'samplingFreqMHz': 20}, \n",
    "                                                                        'NN', \n",
    "                                                                        yVarTest)\n",
    "\n",
    "\n",
    "# Print cancellation performance\n",
    "print('')\n",
    "print('The linear SI cancellation is: {:.2f} dB'.format(yTestPower-yTestLinCancPower))\n",
    "print('The non-linear SI cancellation is: {:.2f} dB'.format(yTestLinCancPower-yTestNonLinCancPower))\n",
    "print('The noise floor is: {:.2f} dBm'.format(noisePower))\n",
    "print('The distance from noise floor is: {:.2f} dB'.format(yTestNonLinCancPower-noisePower))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_RESULTS:\n",
    "    file_name = PATH_EXPORT_RESULTS + 'data_' + DATA_TYPE + '_model_' + MODEL_TYPE + '_linSIC_' + ('yes' if LIN_PREPROCESSING else 'no') + '.pkl'\n",
    "    confirm = input(f'Export as {file_name}? (yes/no)')\n",
    "    if confirm == 'yes':\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump({'y_test': y_test_batched_orig[0, 0, CHANNEL_LEN-1:, 0], 'y_test_lin': y_test_lin[CHANNEL_LEN-1:], 'y_test_nl': yPred/np.sqrt(yVarTest), 'noise': noise, 'yVar': yVarTest, 'chanLen': CHANNEL_LEN}, f)\n",
    "        print('File saved.')\n",
    "    else:\n",
    "        print('File not saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enzner_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
